# harness.py (Generated by Haskell Orchestrator)
import implementation
import pandas as pd
import numpy as np
from scipy import stats
import sys, os

# === Data Contract Enforcement ===
EXPLORATION_DATA_PATH = "/home/karsar/ai-scientist-case-study/data/wine_exploration.csv"
VALIDATION_DATA_PATH = "/home/karsar/ai-scientist-case-study/data/wine_validation.csv"

# Load data within the harness to ensure separation
def load_data(path):
    try:
        return pd.read_csv(path)
    except FileNotFoundError as e:
        print(f"Error loading data: {e}. Check path: {path}")
        sys.exit(1)

# === Statistical Test Implementation (Verified Standardized Methodology) ===
N_REPEATS = 3
N_FOLDS = 10
from sklearn.model_selection import RepeatedKFold

def execute_paired_ttest(data, artifact, baseline_artifact):
    print(f"Executing Paired T-Test ({N_REPEATS} repeats, {N_FOLDS} folds)")
    # Fixed random state for reproducibility of the validation test
    rkf = RepeatedKFold(n_splits=N_FOLDS, n_repeats=N_REPEATS, random_state=42)

    scores_A = []
    scores_B = []

    # Assuming 'target' column is the label. This might need generalization.
    if 'target' not in data.columns:
        print("Error: 'target' column not found in validation data.")
        sys.exit(1)
    X = data.drop('target', axis=1)
    y = data['target']

    for train_index, test_index in rkf.split(X):
        X_train, X_test = X.iloc[train_index], X.iloc[test_index]
        y_train, y_test = y.iloc[train_index], y.iloc[test_index]

        # LLM Implementation provides the evaluation logic for a single fold
        score_A = implementation.evaluate_model(artifact, X_train, y_train, X_test, y_test)
        score_B = implementation.evaluate_model(baseline_artifact, X_train, y_train, X_test, y_test)

        scores_A.append(score_A)
        scores_B.append(score_B)

    # Perform the actual t-test using scipy
    if np.all(np.array(scores_A) == np.array(scores_B)):
        return 1.0 # If scores are identical, no significant difference

    t_stat, p_value = stats.ttest_rel(scores_A, scores_B)

    # We use a one-sided test (Optimized > Baseline)
    if np.mean(scores_A) <= np.mean(scores_B):
        # If mean is not better, p-value is high (not significant in the desired direction)
        return 1.0
    else:
        # For a one-sided test (A > B), we divide the two-sided p-value by 2.
        return p_value / 2.0


# === Execution Phases ===
def run_exploration():
    print("--- Harness: Running Exploration Phase ---")
    data = load_data(EXPLORATION_DATA_PATH)
    # CRITICAL: Only exploration data is passed to the LLM-generated code.
    # We pass a copy to prevent accidental modification of the master data.
    artifact = implementation.optimize(data.copy())
    return artifact

def run_validation(artifact, baseline_artifact):
    print("--- Harness: Running Validation Phase ---")
    data = load_data(VALIDATION_DATA_PATH)
    # Use the harness-defined statistical test.
    p_value = execute_paired_ttest(data.copy(), artifact, baseline_artifact)
    return p_value

# === Main Execution Flow ===
# The harness executes the full cycle: Exploration -> Validation
if __name__ == "__main__":
    # 1. Baseline Definition (LLM provides this via refactoring)
    baseline_artifact = implementation.get_baseline()

    # 2. Exploration Phase (LLM Optimization/Refactoring)
    optimized_artifact = run_exploration()

    # 3. Validation Phase (Statistical Test)
    p_value = run_validation(optimized_artifact, baseline_artifact)

    # Output the P-value for the orchestrator to capture
    print(f"FINAL_P_VALUE:{p_value}")
